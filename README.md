# AtomOra

**Personal Research Intelligence System** â€” an ambient AI colleague that lives in your macOS menubar. Load a PDF, and AtomOra reads it, listens to you, and talks back. Zero-friction, immersive, voice-first.

## What It Does

### Talking Sidebar (interactive)
1. **Load a paper** â€” AtomOra detects the frontmost PDF (Preview/Acrobat), extracts the text, and speaks an initial observation.
2. **Ambient listening** â€” Always-on microphone with VAD (voice activity detection). Just start talking â€” no buttons, no triggers.
3. **Streaming conversation** â€” Your speech is transcribed, streamed to an LLM with the paper context, and spoken back sentence-by-sentence. A floating chat panel shows text in real-time.
4. **Agentic vision** â€” The LLM can extract specific figures from the PDF or screenshot your screen autonomously. Press **âŒ¥S** to manually capture a screenshot.
5. **Interrupt anytime** â€” Press **âŒ¥Space** (Option+Space) to stop the AI mid-sentence and take the floor.

### Daily Paper Briefing (automated)
6. **Multi-source fetching** â€” Pulls recent papers from arXiv, OpenAlex, and Semantic Scholar in parallel.
7. **Smart dedup** â€” Merges duplicates across sources (DOI â†’ arXiv ID â†’ title), prefers journal versions over preprints while keeping arXiv PDF links.
8. **AI filtering** â€” Sonnet 4.5 batch-scores all papers against your research profile (topic relevance + journal prestige) and writes one-line summaries (~$0.10-0.25/day).
9. **Delivery** â€” Slack (Block Kit) with local file path, Markdown archive, and macOS notification (click to open Slack).
10. **Scheduled** â€” Runs daily at 8:00 AM via `launchd`, including weekends.

#### Manual run

```bash
python -m atomora.briefing.run_briefing           # full run (1 day)
python -m atomora.briefing.run_briefing --days 3   # look back 3 days
python -m atomora.briefing.run_briefing --dry-run  # console preview only
python -m atomora.briefing.run_briefing -v         # verbose logging
```

#### Scheduled briefing (launchd)

The briefing runs automatically every day at 8:00 AM via macOS `launchd`. The plist is at `~/Library/LaunchAgents/com.atomora.briefing.plist`.

```bash
# Check status
launchctl list | grep atomora

# Manually trigger a run
launchctl start com.atomora.briefing

# View logs
tail -f data/briefing/launchd.log

# Stop the daily schedule
launchctl unload ~/Library/LaunchAgents/com.atomora.briefing.plist

# Restart the daily schedule
launchctl load ~/Library/LaunchAgents/com.atomora.briefing.plist

# Reload after editing the plist
launchctl unload ~/Library/LaunchAgents/com.atomora.briefing.plist && \
launchctl load ~/Library/LaunchAgents/com.atomora.briefing.plist
```

Briefing results are saved to `data/briefing/YYYY-MM-DD.md`.

AtomOra is not an assistant. It's a research colleague â€” it has opinions, asks probing questions, and tells you what you need to hear.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  macOS Menubar (rumps)                               â”‚
â”‚  ğŸ”¬ğŸ¤                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  Perception                                          â”‚
â”‚  â”œâ”€â”€ window_monitor.py  â€” Detect frontmost PDF       â”‚
â”‚  â”œâ”€â”€ pdf_extractor.py   â€” Extract text (pymupdf)     â”‚
â”‚  â”œâ”€â”€ figure_extractor.pyâ€” Smart figure cropping      â”‚
â”‚  â””â”€â”€ microphone.py      â€” Ambient VAD (silero-vad)   â”‚
â”‚                                                      â”‚
â”‚  STT                                                 â”‚
â”‚  â””â”€â”€ stt.py             â€” whisper.cpp transcription   â”‚
â”‚                                                      â”‚
â”‚  Agent                                               â”‚
â”‚  â”œâ”€â”€ agent_loop.py      â€” Agentic tool-use loop      â”‚
â”‚  â””â”€â”€ tools.py           â€” screenshot, figure extract â”‚
â”‚                                                      â”‚
â”‚  Conversation                                        â”‚
â”‚  â”œâ”€â”€ llm_client.py      â€” Gemini / Claude streaming   â”‚
â”‚  â””â”€â”€ prompts.py         â€” Colleague persona + tools  â”‚
â”‚                                                      â”‚
â”‚  Voice                                               â”‚
â”‚  â””â”€â”€ tts.py             â€” Streaming Edge TTS         â”‚
â”‚                                                      â”‚
â”‚  UI                                                  â”‚
â”‚  â”œâ”€â”€ chat_panel.py      â€” Python â†” Swift bridge      â”‚
â”‚  â””â”€â”€ AtomOraPanel.swift â€” Native floating panel +    â”‚
â”‚                           âŒ¥Space / âŒ¥S hotkeys       â”‚
â”‚                                                      â”‚
â”‚  Briefing                                            â”‚
â”‚  â”œâ”€â”€ sources/            â€” arXiv, OpenAlex, S2       â”‚
â”‚  â”œâ”€â”€ filter.py           â€” Dedup + Sonnet 4.5 filter â”‚
â”‚  â”œâ”€â”€ delivery/           â€” Slack, Markdown, notif    â”‚
â”‚  â””â”€â”€ run_briefing.py     â€” Pipeline orchestrator     â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Voice Pipeline

```
Mic (always on) â†’ VAD speech detect â†’ Record until silence
  â†’ whisper.cpp STT
    â†’ LLM streaming (Claude/Gemini tokens)
      â†’ Sentence accumulator
        â†’ Edge TTS (producer-consumer, queue=2)
          â†’ Speaker
```

The entire pipeline is streaming end-to-end:
- **LLM tokens** arrive and accumulate into sentences
- **TTS generates audio** per-sentence in a background thread while the current sentence plays
- **Chat panel** updates in real-time as tokens arrive
- First word out in **~0.5s** after TTS starts, with zero inter-sentence gaps

See [docs/tts-streaming.md](docs/tts-streaming.md) for architecture details and benchmarks.

### Interrupt (âŒ¥Space)

Press **Option+Space** anywhere to interrupt the AI mid-speech:
- TTS stops immediately
- LLM stops generating tokens
- Producer thread cleans up (drains queue, joins)
- Chat panel shows accumulated text with `[interrupted]` marker
- Microphone resumes listening

The hotkey uses Carbon `RegisterEventHotKey` â€” works system-wide without Accessibility permission.

## Setup

### Prerequisites

- macOS 14+
- Python 3.11+
- [whisper.cpp](https://github.com/ggerganov/whisper.cpp) (`brew install whisper-cpp`)

### Install

```bash
git clone https://github.com/SebDeng/AtomOra.git
cd AtomOra
pip install -r requirements.txt
```

Download the whisper model:
```bash
mkdir -p ~/.cache/whisper
curl -L -o ~/.cache/whisper/ggml-base.bin \
  https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.bin
```

### Compile the Chat Panel (if needed)

The Swift panel binary is pre-compiled, but if you need to rebuild:
```bash
swiftc -o atomora/ui/AtomOraPanel.bin atomora/ui/AtomOraPanel.swift \
  -framework SwiftUI -framework AppKit -framework Carbon
```

### API Keys

Create `atomora/config/secrets.yaml`:

```yaml
gemini:
  api_key: YOUR_GEMINI_API_KEY

anthropic:
  api_key: YOUR_ANTHROPIC_API_KEY

# Optional â€” for daily briefing
openalex:
  email: "your@email.com"         # For polite pool (faster rate limits)
slack:
  webhook_url: ""                  # Slack incoming webhook URL
semanticscholar:
  api_key: ""                      # Optional, increases rate limits
```

### Run

```bash
python -m atomora.main
```

A ğŸ”¬ icon appears in the menubar. Open a PDF in Preview or Acrobat, then click **Load Paper** â€” AtomOra reads the paper, speaks its first observation, and starts listening.

## Configuration

All settings in [`atomora/config/settings.yaml`](atomora/config/settings.yaml):

| Setting | Default | Description |
|---------|---------|-------------|
| `llm.primary` | `claude` | Active LLM (`claude` or `gemini`) |
| `llm.claude.model` | `claude-opus-4-6` | Claude model ID |
| `llm.gemini.model` | `gemini-2.5-pro` | Gemini model ID |
| `voice.tts.engine` | `edge` | TTS engine (`edge` or `macos_say`) |
| `voice.stt.silence_duration` | `1.0` | Seconds of silence to end recording |
| `voice.stt.min_speech_duration` | `0.8` | Minimum speech to process (skip noise) |
| `pdf.max_pages` | `50` | Skip PDFs longer than this |
| `briefing.relevance_threshold` | `0.6` | Minimum score for paper inclusion |
| `briefing.max_papers` | `20` | Max papers per briefing |
| `briefing.arxiv_categories` | `[cond-mat.mtrl-sci, ...]` | arXiv categories to monitor |
| `briefing.s2_queries` | `["hexagonal boron nitride", ...]` | Semantic Scholar search terms |

## Controls

| Control | Action |
|---------|--------|
| **âŒ¥Space** | Interrupt AI speech (global, works from any app) |
| **âŒ¥S** | Capture screenshot and attach to next message |
| ğŸ¤ Listening / ğŸ”‡ Muted | Toggle ambient microphone |
| ğŸ¤ Microphone â–¸ | Select input device |
| ğŸ”Š Speaker â–¸ | Select output device |
| Load Paper (âŒ˜â‡§A) | Detect and load frontmost PDF |
| Show Chat | Toggle floating conversation panel |
| Switch to Gemini/Claude | Swap active LLM |

## Tech Stack

| Component | Technology |
|-----------|------------|
| Menubar app | [rumps](https://github.com/jaredks/rumps) |
| PDF extraction | [pymupdf](https://pymupdf.readthedocs.io/) |
| Window detection | PyObjC (NSWorkspace) |
| VAD | [silero-vad](https://github.com/snakers4/silero-vad) |
| STT | [whisper.cpp](https://github.com/ggerganov/whisper.cpp) |
| LLM (interactive) | Claude Opus 4.6 / Gemini 2.5 Pro (streaming) |
| LLM (briefing filter) | Claude Sonnet 4.5 (batch scoring) |
| TTS | [Edge TTS](https://github.com/rany2/edge-tts) (sentence-level streaming) |
| Paper sources | [arxiv](https://pypi.org/project/arxiv/), [pyalex](https://pypi.org/project/pyalex/), [semanticscholar](https://pypi.org/project/semanticscholar/) |
| Slack delivery | requests (incoming webhook, Block Kit) |
| Chat panel | SwiftUI (NSPanel, dark ultra-thin material) |
| Global hotkey | Carbon RegisterEventHotKey |
| Audio I/O | [sounddevice](https://python-sounddevice.readthedocs.io/) |

## Docs

- [TTS Streaming Architecture](docs/tts-streaming.md) â€” sentence-level streaming pipeline, benchmarks, configuration

## License

Private â€” not yet open source.
