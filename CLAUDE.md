# AtomOra â€” Personal Research Intelligence System

> An ambient AI colleague that reads papers with you. Load a PDF, just talk.
> AtomOra listens continuously, responds, and talks back. Zero-friction, voice-first.

## Identity

AtomOra is NOT an assistant. It is an **ambient AI colleague** that shares your
research context. It has opinions, expresses uncertainty, proactively comments,
and tells you what you need to hear.

The persona is the **Donna Paulsen model**: someone who knows your entire context,
has her own judgment, and doesn't wait to be asked. Bilingual Chinese-English,
naturally code-switching.

## Current State (Phase 3.1 â€” Agentic Vision)

Phase 1 (Talking Sidebar) is complete. Phase 3.1 adds agentic tool use:

1. Load a PDF (detected from frontmost window)
2. AI pre-reads and speaks initial observations (interruptible)
3. Ambient microphone listens continuously via VAD
4. Speech â†’ STT â†’ **Agent Loop** (LLM + tool calls) â†’ TTS â†’ Speaker
5. LLM can **autonomously screenshot** the screen to analyze figures/charts
6. User can **manually capture** screenshots via **âŒ¥S** hotkey
7. Floating chat panel shows conversation + tool execution in real-time

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  macOS Menubar (rumps)                               â”‚
â”‚  ğŸ”¬ğŸ¤                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  Perception                                          â”‚
â”‚  â”œâ”€â”€ window_monitor.py  â€” Detect frontmost PDF +     â”‚
â”‚  â”‚                        get window ID for capture   â”‚
â”‚  â”œâ”€â”€ pdf_extractor.py   â€” Extract text (pymupdf)     â”‚
â”‚  â””â”€â”€ microphone.py      â€” Ambient VAD (silero-vad)   â”‚
â”‚                                                      â”‚
â”‚  STT                                                 â”‚
â”‚  â””â”€â”€ stt.py             â€” whisper.cpp transcription   â”‚
â”‚                                                      â”‚
â”‚  Agent                                               â”‚
â”‚  â”œâ”€â”€ agent_loop.py      â€” Agentic tool-use loop      â”‚
â”‚  â””â”€â”€ tools.py           â€” Tool registry + executors   â”‚
â”‚                                                      â”‚
â”‚  Conversation                                        â”‚
â”‚  â”œâ”€â”€ llm_client.py      â€” Gemini / Claude streaming   â”‚
â”‚  â”‚                        (text + tool-aware + vision) â”‚
â”‚  â””â”€â”€ prompts.py         â€” Colleague persona + tools   â”‚
â”‚                                                      â”‚
â”‚  Voice                                               â”‚
â”‚  â””â”€â”€ tts.py             â€” Streaming Edge TTS         â”‚
â”‚                                                      â”‚
â”‚  UI                                                  â”‚
â”‚  â”œâ”€â”€ chat_panel.py      â€” Python â†” Swift bridge      â”‚
â”‚  â””â”€â”€ AtomOraPanel.swift â€” Native floating panel +    â”‚
â”‚                           âŒ¥Space interrupt +         â”‚
â”‚                           âŒ¥S screenshot hotkey       â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### End-to-End Streaming Pipeline

```
Mic (always on)
  â†’ silero-vad speech detect
    â†’ Record until silence (1.0s)
      â†’ whisper.cpp STT
        â†’ Agent Loop (LLM + tool execution)
          â”œâ”€â”€ LLM streams tokens â†’ text chunks
          â””â”€â”€ LLM calls tool (take_screenshot)
                â†’ screencapture â†’ base64 PNG â†’ back to LLM
                â†’ LLM analyzes image â†’ text chunks
          â†’ Sentence accumulator (regex boundary split)
            â†’ TTS producer-consumer (Edge TTS, queue=2)
              â†’ Speaker (sounddevice)
                â†’ Chat panel (real-time text via Swift stdin)
```

Key design decisions:
- **Agentic loop**: LLM can call tools (screenshot) autonomously, like Claude Code
- **Agent yields text only**: tool calls handled transparently inside the loop
- **LLM streams tokens** â†’ accumulated into sentences â†’ TTS processes per-sentence
- **Mic paused during TTS** (sleep, not drain) to avoid audio contention
- **Overflow detection** on mic resume to skip stale audio
- **Interrupt via âŒ¥Space** â€” Carbon RegisterEventHotKey (no Accessibility permission needed)
- **Screenshot via âŒ¥S** â€” user-initiated capture, attached to next voice message

### Interrupt Flow (âŒ¥Space)

1. Swift panel receives Carbon hotkey event
2. Writes `{"event":"interrupt"}` to stdout
3. Python ChatPanel reads stdout, fires `_on_interrupt()` callback
4. `_on_interrupt()` sets `_interrupted = True`, calls `tts.stop()`
5. `tts.stop()` sets `_speaking = False`, calls `sd.stop()`
6. Consumer loop breaks â†’ drains queue â†’ joins producer thread
7. Generator checks `_interrupted` â†’ stops pulling LLM tokens
8. Chat panel shows accumulated text + `[interrupted]` marker

## Tech Stack (Actual Implementation)

| Component | Technology | Notes |
|-----------|-----------|-------|
| Language | Python 3.11+ | ML + audio + macOS ecosystem |
| Menubar | rumps | macOS menubar daemon |
| macOS APIs | pyobjc | NSWorkspace for PDF detection |
| PDF extraction | pymupdf (fitz) | Fast, reliable |
| VAD | silero-vad | Voice activity detection, 512 samples min at 16kHz |
| STT | whisper.cpp (whisper-cli) | Local, ggml-base model |
| LLM (primary) | Claude Opus 4.6 API | Streaming via anthropic SDK |
| LLM (secondary) | Gemini 2.5 Pro API | Streaming via google-genai SDK |
| TTS | Edge TTS (edge-tts) | Cloud neural TTS, sentence-level streaming |
| TTS fallback | macOS `say` | Offline |
| Chat panel | SwiftUI (NSPanel) | Dark ultra-thin material, stdin/stdout IPC |
| Global hotkey | Carbon RegisterEventHotKey | âŒ¥Space interrupt, no Accessibility needed |
| Audio I/O | sounddevice + soundfile | Input (mic) and output (TTS playback) |

## Project Structure

```
atomora/
â”œâ”€â”€ main.py                    # Entry point, menubar app, streaming pipeline
â”œâ”€â”€ perception/
â”‚   â”œâ”€â”€ microphone.py          # Ambient VAD listening (silero-vad + sounddevice)
â”‚   â”œâ”€â”€ window_monitor.py      # Active window/PDF detection + screenshot (pyobjc)
â”‚   â””â”€â”€ pdf_extractor.py       # Text extraction (pymupdf)
â”œâ”€â”€ stt.py                     # whisper.cpp STT wrapper
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ agent_loop.py          # Agentic tool-use loop (LLM â†’ tool â†’ LLM)
â”‚   â””â”€â”€ tools.py               # Tool definitions + executors (screenshot, etc.)
â”œâ”€â”€ conversation/
â”‚   â”œâ”€â”€ llm_client.py          # Gemini + Claude streaming (text + tools + vision)
â”‚   â””â”€â”€ prompts.py             # System prompts (colleague persona + tools)
â”œâ”€â”€ voice/
â”‚   â””â”€â”€ tts.py                 # Streaming Edge TTS (producer-consumer pipeline)
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ chat_panel.py          # Python â†” Swift bridge (stdin/stdout JSON)
â”‚   â”œâ”€â”€ AtomOraPanel.swift     # Native SwiftUI panel + âŒ¥Space/âŒ¥S hotkeys
â”‚   â””â”€â”€ AtomOraPanel.bin       # Compiled Swift binary
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.yaml          # General settings
â”‚   â””â”€â”€ secrets.yaml           # API keys (gitignored)
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ tts-streaming.md       # TTS architecture and benchmarks
â”œâ”€â”€ CLAUDE.md                  # This file
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## Key Implementation Details

### Microphone (microphone.py)
- silero-vad requires **512 samples minimum** at 16kHz (not 480)
- Retry logic with backoff on audio device errors (PaMacCore)
- During TTS pause: **sleep** (not read) to avoid audio hardware contention
- After resume: **overflow detection** skips stale buffered audio
- VAD state reset on each new listen session

### LLM Streaming (llm_client.py)
- `chat_stream()` generator with `try/finally` for history management
- Claude: `client.messages.stream()` â†’ `stream.text_stream`
- Gemini: `client.models.generate_content_stream()`
- Partial responses saved to history on interrupt

### TTS Streaming (tts.py)
- Producer-consumer with `Queue(maxsize=2)`
- Producer generates audio per-sentence in background thread
- Consumer plays audio via sounddevice on calling thread
- On interrupt: producer checks `_speaking` after each edge_generate, drains queue, joins thread
- Language detection on first sentence (Chinese â‰¥20% â†’ Chinese voice)
- See [docs/tts-streaming.md](docs/tts-streaming.md) for benchmarks

### Chat Panel (AtomOraPanel.swift)
- Native SwiftUI NSPanel with `.ultraThinMaterial` dark glass effect
- Floating, always-on-top, joins all spaces
- Pythonâ†’Swift: JSON lines on stdin (append, update_last, clear, show, hide)
- Swiftâ†’Python: JSON lines on stdout (interrupt events)
- Carbon `RegisterEventHotKey` for âŒ¥Space global hotkey

### Agent Loop (agent/agent_loop.py)
- **Agentic tool-use pattern** modeled after Claude Code
- Wraps LLMClient, yields only text chunks â€” tool calls handled transparently
- Loop: LLM streams â†’ detect ToolCallRequest â†’ execute tool â†’ feed result â†’ LLM streams again
- Max tool rounds configurable (default 5) to prevent infinite loops
- Interrupt check between each tool call and stream event
- Tools defined in `agent/tools.py` with Claude-format schemas, converted to Gemini at runtime

### Vision / Screenshot (agent/tools.py)
- `take_screenshot`: captures frontmost window via `screencapture -l <windowid>`
- Window ID from `get_frontmost_window_id()` in window_monitor.py (CGWindowListCopyWindowInfo)
- Resizes images >1920px wide via `sips` to control API cost
- Returns base64 PNG as image content block in Claude API format
- LLM decides autonomously when to screenshot (proactive tool use)
- User can also trigger via **âŒ¥S** hotkey â†’ image attached to next voice message

### Tool-Aware LLM Streaming (llm_client.py)
- `chat_stream_with_tools()`: iterates over raw stream events (not text_stream)
- Claude: detects `content_block_start` with `type="tool_use"`, accumulates JSON from `input_json_delta`
- Gemini: checks `part.function_call` in stream chunks
- History supports both string content and structured content blocks (tool_use, tool_result, images)
- `_messages_to_gemini_contents()`: converts Claude-format messages to Gemini Content objects

### Sentence Accumulator (main.py `_stream_and_speak`)
- LLM tokens accumulated into `sentence_buf`
- Split on `SENTENCE_BOUNDARY` regex: `(?<=[.!?])\s+|(?<=[ã€‚ï¼ï¼Ÿï¼›])`
- Markdown stripped before TTS via `_strip_for_speech()`
- Chat panel updated in real-time with raw LLM text

## Conventions

- Python 3.11+, type hints where helpful
- Config via YAML files in `config/`
- Secrets in `config/secrets.yaml` (gitignored)
- Prefer composition over inheritance
- Modules are loosely coupled â€” perception, conversation, voice, ui are independent
- Thread safety: flags (`_speaking`, `_interrupted`, `_processing`) checked across threads via Python GIL
- Daemon threads for background work (producer, mic loop, hotkey reader)

## Colleague Persona

- Has opinions. Expresses uncertainty. Proactively comments.
- Does NOT say "How can I help you?" â€” comments like a colleague would
- Bilingual: matches the user's language mix naturally
- Domain: atomic-scale physics, photonics, materials science
- Voice output: concise, conversational, no markdown formatting

## Future Phases

- **Phase 2**: Ambient context awareness (attention state, address detection)
- **Phase 3.2**: Daily paper briefing (arXiv RSS â†’ LLM filter â†’ voice summary)
- **Phase 4**: Knowledge graph, long-term memory across sessions
